import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
import numpy as np

st.set_page_config(page_title="RLHF - Reinforcement Learning from Human Feedback", page_icon="üéØ", layout="wide")

st.title("üéØ Reinforcement Learning from Human Feedback (RLHF)")
st.markdown("### Aligning Language Models with Human Preferences")

# Overview
st.header("üéØ Overview")
st.markdown("""
Reinforcement Learning from Human Feedback (RLHF) is a technique used to train language models 
to produce outputs that align with human preferences and values. It has been crucial in creating 
helpful, harmless, and honest AI assistants like ChatGPT and Claude.
""")

# Core concepts
st.header("üß† Core Concepts")

concept_tabs = st.tabs([
    "üîÑ RLHF Process",
    "üèÜ Reward Modeling", 
    "üéÆ Policy Optimization",
    "üìä Evaluation Methods"
])

with concept_tabs[0]:
    st.subheader("üîÑ The RLHF Process")
    
    st.markdown("""
    RLHF consists of three main stages that transform a base language model into 
    an aligned assistant that follows human preferences.
    """)
    
    # RLHF stages
    rlhf_stages = [
        {
            "stage": "Stage 1: Supervised Fine-tuning (SFT)",
            "description": "Train the model on high-quality demonstration data",
            "inputs": [
                "Pre-trained base model",
                "Human-written demonstrations",
                "High-quality instruction-response pairs"
            ],
            "process": [
                "Collect demonstrations of desired behavior",
                "Fine-tune base model on demonstrations",
                "Create initial aligned model",
                "Validate performance on held-out data"
            ],
            "outputs": [
                "SFT model with basic alignment",
                "Improved instruction following",
                "Better response quality baseline"
            ],
            "implementation": """
# Supervised Fine-tuning Implementation
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.utils.data import DataLoader

class SFTTrainer:
    def __init__(self, model_name, learning_rate=1e-5):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)
        
        # Add padding token if needed
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def prepare_data(self, demonstrations):
        # Format demonstrations as instruction-response pairs
        formatted_data = []
        
        for demo in demonstrations:
            # Create input-output pair
            input_text = f"Human: {demo['instruction']}\\n\\nAssistant: "
            full_text = input_text + demo['response'] + self.tokenizer.eos_token
            
            # Tokenize
            tokens = self.tokenizer.encode(full_text, truncation=True, max_length=2048)
            input_len = len(self.tokenizer.encode(input_text))
            
            formatted_data.append({
                'input_ids': tokens,
                'labels': [-100] * (input_len - 1) + tokens[input_len-1:],  # Mask instruction tokens
                'attention_mask': [1] * len(tokens)
            })
        
        return formatted_data
    
    def train_step(self, batch):
        self.optimizer.zero_grad()
        
        # Forward pass
        outputs = self.model(
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask'],
            labels=batch['labels']
        )
        
        loss = outputs.loss
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        
        return loss.item()
    
    def train(self, demonstrations, epochs=3, batch_size=4):
        data = self.prepare_data(demonstrations)
        dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)
        
        self.model.train()
        total_loss = 0
        
        for epoch in range(epochs):
            epoch_loss = 0
            
            for batch in dataloader:
                # Move to device
                batch = {k: v.to(self.model.device) for k, v in batch.items()}
                
                loss = self.train_step(batch)
                epoch_loss += loss
            
            avg_loss = epoch_loss / len(dataloader)
            print(f"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}")
            total_loss += avg_loss
        
        return total_loss / epochs

# Example usage
demonstrations = [
    {
        "instruction": "Explain what photosynthesis is",
        "response": "Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen..."
    },
    {
        "instruction": "How do I bake a chocolate cake?",
        "response": "To bake a chocolate cake, you'll need flour, sugar, cocoa powder, eggs, butter, and baking powder..."
    }
]

trainer = SFTTrainer("gpt2-medium")
avg_loss = trainer.train(demonstrations)
"""
        },
        {
            "stage": "Stage 2: Reward Model Training",
            "description": "Train a model to predict human preferences",
            "inputs": [
                "SFT model outputs",
                "Human preference comparisons",
                "Ranking data for response pairs"
            ],
            "process": [
                "Generate response pairs from SFT model",
                "Collect human preference rankings",
                "Train reward model on preference data",
                "Validate reward model accuracy"
            ],
            "outputs": [
                "Reward model that predicts human preferences",
                "Scalar reward scores for any response",
                "Preference prediction capability"
            ],
            "implementation": """
# Reward Model Training Implementation
import torch.nn.functional as F
from sklearn.metrics import accuracy_score

class RewardModel(nn.Module):
    def __init__(self, base_model_name):
        super().__init__()
        self.base_model = AutoModelForCausalLM.from_pretrained(base_model_name)
        
        # Replace language modeling head with reward head
        hidden_size = self.base_model.config.hidden_size
        self.reward_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )
        
        # Freeze base model parameters (optional)
        for param in self.base_model.parameters():
            param.requires_grad = False
    
    def forward(self, input_ids, attention_mask=None):
        # Get hidden states from base model
        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        # Use hidden state at the last token position
        hidden_states = outputs.hidden_states[-1]  # Last layer
        
        if attention_mask is not None:
            # Get last non-padded token for each sequence
            last_positions = attention_mask.sum(dim=1) - 1
            sequence_hidden = hidden_states[range(len(hidden_states)), last_positions]
        else:
            sequence_hidden = hidden_states[:, -1]  # Last token
        
        # Compute reward score
        reward = self.reward_head(sequence_hidden)
        return reward.squeeze(-1)

class RewardModelTrainer:
    def __init__(self, model_name, learning_rate=1e-5):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = RewardModel(model_name)
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def prepare_comparison_data(self, comparisons):
        # Format preference comparison data
        formatted_data = []
        
        for comparison in comparisons:
            prompt = comparison['prompt']
            chosen = comparison['chosen_response']
            rejected = comparison['rejected_response']
            
            # Tokenize chosen and rejected responses
            chosen_text = f"Human: {prompt}\\n\\nAssistant: {chosen}"
            rejected_text = f"Human: {prompt}\\n\\nAssistant: {rejected}"
            
            chosen_tokens = self.tokenizer(
                chosen_text, 
                truncation=True, 
                max_length=1024, 
                padding=True, 
                return_tensors="pt"
            )
            
            rejected_tokens = self.tokenizer(
                rejected_text,
                truncation=True,
                max_length=1024,
                padding=True,
                return_tensors="pt"
            )
            
            formatted_data.append({
                'chosen_input_ids': chosen_tokens['input_ids'].squeeze(),
                'chosen_attention_mask': chosen_tokens['attention_mask'].squeeze(),
                'rejected_input_ids': rejected_tokens['input_ids'].squeeze(),
                'rejected_attention_mask': rejected_tokens['attention_mask'].squeeze()
            })
        
        return formatted_data
    
    def compute_loss(self, chosen_rewards, rejected_rewards):
        # Bradley-Terry model loss
        # P(chosen > rejected) = sigmoid(reward_chosen - reward_rejected)
        logits = chosen_rewards - rejected_rewards
        loss = -F.logsigmoid(logits).mean()
        return loss
    
    def train_step(self, batch):
        self.optimizer.zero_grad()
        
        # Get rewards for chosen and rejected responses
        chosen_rewards = self.model(
            batch['chosen_input_ids'],
            batch['chosen_attention_mask']
        )
        
        rejected_rewards = self.model(
            batch['rejected_input_ids'],
            batch['rejected_attention_mask']
        )
        
        # Compute loss
        loss = self.compute_loss(chosen_rewards, rejected_rewards)
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        self.optimizer.step()
        
        # Compute accuracy (how often chosen > rejected)
        with torch.no_grad():
            correct = (chosen_rewards > rejected_rewards).float()
            accuracy = correct.mean().item()
        
        return loss.item(), accuracy
    
    def train(self, comparisons, epochs=3, batch_size=4):
        data = self.prepare_comparison_data(comparisons)
        dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)
        
        self.model.train()
        
        for epoch in range(epochs):
            total_loss = 0
            total_accuracy = 0
            
            for batch in dataloader:
                batch = {k: v.to(self.model.device) for k, v in batch.items()}
                
                loss, accuracy = self.train_step(batch)
                total_loss += loss
                total_accuracy += accuracy
            
            avg_loss = total_loss / len(dataloader)
            avg_accuracy = total_accuracy / len(dataloader)
            
            print(f"Epoch {epoch + 1}")
            print(f"  Loss: {avg_loss:.4f}")
            print(f"  Accuracy: {avg_accuracy:.4f}")

# Example preference data
comparisons = [
    {
        "prompt": "Explain quantum computing",
        "chosen_response": "Quantum computing uses quantum mechanical phenomena like superposition and entanglement to process information...",
        "rejected_response": "Quantum computing is just really fast regular computing with quantum stuff."
    }
]

reward_trainer = RewardModelTrainer("gpt2-medium")
reward_trainer.train(comparisons)
"""
        },
        {
            "stage": "Stage 3: Reinforcement Learning",
            "description": "Optimize the policy using the reward model",
            "inputs": [
                "SFT model as initial policy",
                "Trained reward model",
                "Training prompts"
            ],
            "process": [
                "Generate responses from current policy",
                "Score responses with reward model",
                "Update policy to maximize rewards",
                "Apply KL divergence constraint"
            ],
            "outputs": [
                "Final RLHF-trained model",
                "Aligned behavior with human preferences",
                "Improved helpfulness and safety"
            ],
            "implementation": """
# PPO (Proximal Policy Optimization) for RLHF
import torch.distributions as dist

class PPOTrainer:
    def __init__(self, policy_model, reward_model, ref_model, kl_coeff=0.1):
        self.policy = policy_model
        self.reward_model = reward_model
        self.ref_model = ref_model  # Reference model for KL penalty
        self.kl_coeff = kl_coeff
        
        self.optimizer = torch.optim.AdamW(self.policy.parameters(), lr=1e-6)
        
    def generate_responses(self, prompts, max_length=512, num_return_sequences=1):
        responses = []
        
        for prompt in prompts:
            # Tokenize prompt
            inputs = self.policy.tokenizer(
                prompt, 
                return_tensors="pt", 
                padding=True, 
                truncation=True
            )
            
            # Generate response
            with torch.no_grad():
                outputs = self.policy.generate(
                    **inputs,
                    max_length=max_length,
                    num_return_sequences=num_return_sequences,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=self.policy.tokenizer.eos_token_id
                )
            
            # Decode response
            response_text = self.policy.tokenizer.decode(
                outputs[0], 
                skip_special_tokens=True
            )
            
            responses.append({
                'prompt': prompt,
                'response': response_text,
                'input_ids': outputs[0]
            })
        
        return responses
    
    def compute_rewards(self, responses):
        rewards = []
        
        for response in responses:
            # Get reward from reward model
            full_text = response['prompt'] + response['response']
            inputs = self.reward_model.tokenizer(
                full_text,
                return_tensors="pt",
                truncation=True,
                max_length=1024
            )
            
            with torch.no_grad():
                reward = self.reward_model(**inputs)
                rewards.append(reward.item())
        
        return torch.tensor(rewards)
    
    def compute_kl_penalty(self, responses):
        kl_penalties = []
        
        for response in responses:
            input_ids = response['input_ids'].unsqueeze(0)
            
            # Get log probabilities from current policy
            with torch.no_grad():
                policy_outputs = self.policy(input_ids, labels=input_ids)
                policy_logprobs = -policy_outputs.loss
            
            # Get log probabilities from reference model
            with torch.no_grad():
                ref_outputs = self.ref_model(input_ids, labels=input_ids)
                ref_logprobs = -ref_outputs.loss
            
            # Compute KL divergence
            kl = policy_logprobs - ref_logprobs
            kl_penalties.append(kl.item())
        
        return torch.tensor(kl_penalties)
    
    def ppo_update(self, responses, rewards, advantages, epochs=4):
        # Convert responses to training data
        input_ids = []
        attention_masks = []
        old_logprobs = []
        
        for response in responses:
            tokens = response['input_ids']
            input_ids.append(tokens)
            attention_masks.append(torch.ones_like(tokens))
            
            # Compute old log probabilities
            with torch.no_grad():
                outputs = self.policy(tokens.unsqueeze(0), labels=tokens.unsqueeze(0))
                old_logprobs.append(-outputs.loss)
        
        input_ids = torch.stack(input_ids)
        attention_masks = torch.stack(attention_masks)
        old_logprobs = torch.stack(old_logprobs)
        
        # PPO updates
        for _ in range(epochs):
            # Forward pass
            outputs = self.policy(input_ids, attention_mask=attention_masks, labels=input_ids)
            new_logprobs = -outputs.loss
            
            # Compute ratio
            ratio = torch.exp(new_logprobs - old_logprobs)
            
            # Compute PPO loss
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 0.8, 1.2) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # Backward pass
            self.optimizer.zero_grad()
            policy_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=1.0)
            self.optimizer.step()
        
        return policy_loss.item()
    
    def train_step(self, prompts):
        # Generate responses
        responses = self.generate_responses(prompts)
        
        # Compute rewards
        rewards = self.compute_rewards(responses)
        
        # Compute KL penalty
        kl_penalties = self.compute_kl_penalty(responses)
        
        # Final rewards with KL penalty
        final_rewards = rewards - self.kl_coeff * kl_penalties
        
        # Compute advantages (simplified - just use rewards)
        advantages = final_rewards - final_rewards.mean()
        advantages = advantages / (advantages.std() + 1e-8)
        
        # PPO update
        policy_loss = self.ppo_update(responses, final_rewards, advantages)
        
        return {
            'policy_loss': policy_loss,
            'avg_reward': rewards.mean().item(),
            'avg_kl': kl_penalties.mean().item()
        }

# Example RLHF training
prompts = [
    "Explain the benefits of renewable energy",
    "How can I improve my public speaking skills?",
    "What are the main causes of climate change?"
]

ppo_trainer = PPOTrainer(sft_model, reward_model, reference_model)

# Training loop
for step in range(1000):
    metrics = ppo_trainer.train_step(prompts)
    
    if step % 100 == 0:
        print(f"Step {step}")
        print(f"  Policy Loss: {metrics['policy_loss']:.4f}")
        print(f"  Avg Reward: {metrics['avg_reward']:.4f}")
        print(f"  Avg KL: {metrics['avg_kl']:.4f}")
"""
        }
    ]
    
    for stage in rlhf_stages:
        with st.expander(f"üîÑ {stage['stage']}"):
            st.markdown(stage['description'])
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.markdown("**Inputs:**")
                for inp in stage['inputs']:
                    st.markdown(f"‚Ä¢ {inp}")
            
            with col2:
                st.markdown("**Process:**")
                for process in stage['process']:
                    st.markdown(f"‚Ä¢ {process}")
            
            with col3:
                st.markdown("**Outputs:**")
                for output in stage['outputs']:
                    st.markdown(f"‚Ä¢ {output}")
            
            st.markdown("**Implementation Example:**")
            st.code(stage['implementation'], language='python')

with concept_tabs[1]:
    st.subheader("üèÜ Reward Modeling")
    
    st.markdown("""
    Reward modeling is the core of RLHF, translating human preferences into 
    scalar reward signals that can guide reinforcement learning.
    """)
    
    reward_topics = st.tabs([
        "üìä Preference Collection",
        "üß† Model Architecture",
        "üìà Training Strategies",
        "‚öñÔ∏è Challenges & Solutions"
    ])
    
    with reward_topics[0]:
        st.markdown("### üìä Preference Collection Methods")
        
        collection_methods = [
            {
                "method": "Pairwise Comparisons",
                "description": "Humans compare two responses and choose the better one",
                "advantages": [
                    "Easier for humans than absolute scoring",
                    "More reliable and consistent judgments",
                    "Captures relative preferences well",
                    "Scalable with crowd workers"
                ],
                "disadvantages": [
                    "Requires many comparisons for coverage",
                    "May miss fine-grained quality differences",
                    "Intransitive preferences possible",
                    "Limited to binary choices"
                ],
                "implementation": """
# Pairwise Comparison Data Collection
class PreferenceCollector:
    def __init__(self, model, prompts):
        self.model = model
        self.prompts = prompts
        self.comparisons = []
    
    def generate_response_pairs(self, prompt, num_pairs=5):
        pairs = []
        
        for _ in range(num_pairs):
            # Generate two different responses
            response_a = self.model.generate(
                prompt, 
                temperature=0.8, 
                max_tokens=200
            )
            response_b = self.model.generate(
                prompt, 
                temperature=0.8, 
                max_tokens=200
            )
            
            pairs.append({
                'prompt': prompt,
                'response_a': response_a,
                'response_b': response_b
            })
        
        return pairs
    
    def collect_human_preferences(self, pairs):
        # In practice, this would interface with human annotators
        # Here we simulate the interface
        labeled_pairs = []
        
        for pair in pairs:
            print(f"Prompt: {pair['prompt']}")
            print(f"Response A: {pair['response_a']}")
            print(f"Response B: {pair['response_b']}")
            
            # Human annotator chooses A, B, or tie
            choice = input("Which response is better? (A/B/T for tie): ").upper()
            
            if choice == 'A':
                chosen = pair['response_a']
                rejected = pair['response_b']
            elif choice == 'B':
                chosen = pair['response_b']
                rejected = pair['response_a']
            else:  # Tie
                continue  # Skip ties for simplicity
            
            labeled_pairs.append({
                'prompt': pair['prompt'],
                'chosen': chosen,
                'rejected': rejected,
                'annotator_confidence': self.get_confidence_rating()
            })
        
        return labeled_pairs
    
    def get_confidence_rating(self):
        # Get annotator confidence in their judgment
        confidence = input("How confident are you? (1-5): ")
        return int(confidence) if confidence.isdigit() else 3
    
    def validate_consistency(self, comparisons):
        # Check for inconsistent preferences
        inconsistencies = []
        
        for i, comp1 in enumerate(comparisons):
            for j, comp2 in enumerate(comparisons[i+1:], i+1):
                if (comp1['prompt'] == comp2['prompt'] and
                    comp1['chosen'] == comp2['rejected'] and
                    comp1['rejected'] == comp2['chosen']):
                    inconsistencies.append((i, j))
        
        return inconsistencies
"""
            },
            {
                "method": "Absolute Scoring",
                "description": "Humans rate responses on absolute quality scales",
                "advantages": [
                    "Provides absolute quality measures",
                    "Can capture fine-grained differences",
                    "Efficient for single response evaluation",
                    "Allows for multi-dimensional scoring"
                ],
                "disadvantages": [
                    "Subjective scale interpretation",
                    "Annotator bias and drift",
                    "Harder to achieve consistency",
                    "Scale boundary effects"
                ],
                "implementation": """
# Absolute Scoring Data Collection
class AbsoluteScorer:
    def __init__(self, scoring_dimensions):
        self.dimensions = scoring_dimensions
        # e.g., ['helpfulness', 'accuracy', 'safety', 'clarity']
    
    def score_response(self, prompt, response):
        scores = {}
        
        print(f"Prompt: {prompt}")
        print(f"Response: {response}")
        print("\\nPlease rate the response on each dimension (1-10):")
        
        for dimension in self.dimensions:
            score = input(f"{dimension.capitalize()}: ")
            scores[dimension] = int(score) if score.isdigit() else 5
        
        # Compute overall score
        overall_score = sum(scores.values()) / len(scores)
        scores['overall'] = overall_score
        
        return scores
    
    def collect_scores(self, prompt_response_pairs):
        scored_data = []
        
        for pair in prompt_response_pairs:
            scores = self.score_response(pair['prompt'], pair['response'])
            
            scored_data.append({
                'prompt': pair['prompt'],
                'response': pair['response'],
                'scores': scores,
                'timestamp': datetime.now().isoformat()
            })
        
        return scored_data
    
    def normalize_scores(self, scored_data, annotator_id):
        # Normalize scores to account for annotator bias
        all_scores = [item['scores']['overall'] for item in scored_data]
        mean_score = np.mean(all_scores)
        std_score = np.std(all_scores)
        
        normalized_data = []
        for item in scored_data:
            normalized_overall = (item['scores']['overall'] - mean_score) / std_score
            
            normalized_item = item.copy()
            normalized_item['scores']['normalized_overall'] = normalized_overall
            normalized_item['annotator_stats'] = {
                'mean': mean_score,
                'std': std_score,
                'annotator_id': annotator_id
            }
            
            normalized_data.append(normalized_item)
        
        return normalized_data
"""
            },
            {
                "method": "Constitutional AI",
                "description": "Use AI to evaluate responses against constitutional principles",
                "advantages": [
                    "Scalable automated evaluation",
                    "Consistent application of principles",
                    "Reduces human annotation burden",
                    "Can incorporate complex criteria"
                ],
                "disadvantages": [
                    "Limited by AI evaluator capability",
                    "May perpetuate model biases",
                    "Less diverse than human judgment",
                    "Requires careful prompt engineering"
                ],
                "implementation": """
# Constitutional AI Evaluation
class ConstitutionalEvaluator:
    def __init__(self, evaluator_model, constitution):
        self.evaluator = evaluator_model
        self.constitution = constitution
        # Constitution: list of principles/criteria
    
    def evaluate_response(self, prompt, response):
        evaluations = {}
        
        for principle in self.constitution:
            eval_prompt = f'''
Evaluate the following response according to this principle: {principle}

Original Prompt: {prompt}
Response: {response}

Rate the response on how well it follows this principle (1-10):
'''
            
            evaluation = self.evaluator.generate(eval_prompt)
            try:
                score = float(evaluation.strip())
                evaluations[principle] = score
            except ValueError:
                evaluations[principle] = 5.0  # Default neutral score
        
        return evaluations
"""
            }
        ]
        
        for method in evaluation_methods:
            with st.expander(f"üîç {method['method']}"):
                st.markdown(method['description'])
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**Advantages:**")
                    for advantage in method['advantages']:
                        st.markdown(f"‚Ä¢ {advantage}")
                
                with col2:
                    st.markdown("**Disadvantages:**")
                    for disadvantage in method['disadvantages']:
                        st.markdown(f"‚Ä¢ {disadvantage}")
                
                st.markdown("**Implementation:**")
                st.code(method['implementation'], language='python')

# Advanced RLHF techniques
st.header("üöÄ Advanced RLHF Techniques")

advanced_tabs = st.tabs([
    "üîÑ Iterative RLHF",
    "üéØ Multi-Objective RLHF", 
    "üìä Online Learning",
    "üîß Debugging RLHF"
])

with advanced_tabs[0]:
    st.subheader("üîÑ Iterative RLHF")
    
    st.markdown("""
    Iterative RLHF involves multiple rounds of human feedback collection and model training
    to continuously improve model performance.
    """)
    
    iterative_process = [
        "Initial model training with supervised fine-tuning",
        "Collect human preferences on model outputs", 
        "Train reward model on preference data",
        "Run PPO to optimize policy using reward model",
        "Deploy updated model and collect new feedback",
        "Retrain reward model with additional data",
        "Continue cycle until satisfactory performance"
    ]
    
    for i, step in enumerate(iterative_process, 1):
        st.markdown(f"{i}. {step}")
    
    st.markdown("### üíª Iterative RLHF Implementation")
    
    st.code("""
class IterativeRLHF:
    def __init__(self, base_model, tokenizer):
        self.base_model = base_model
        self.tokenizer = tokenizer
        self.reward_model = None
        self.policy_model = None
        self.iteration = 0
        
    def run_iteration(self, prompts, human_feedback_func):
        print(f"Starting RLHF iteration {self.iteration + 1}")
        
        # Step 1: Generate responses with current model
        responses = self.generate_responses(prompts)
        
        # Step 2: Collect human feedback
        preference_data = human_feedback_func(prompts, responses)
        
        # Step 3: Update reward model
        if self.reward_model is None:
            self.reward_model = self.train_initial_reward_model(preference_data)
        else:
            self.reward_model = self.update_reward_model(preference_data)
        
        # Step 4: Run PPO optimization
        self.policy_model = self.run_ppo_optimization()
        
        # Step 5: Evaluate improvements
        evaluation_metrics = self.evaluate_model()
        
        self.iteration += 1
        
        return evaluation_metrics
    
    def generate_responses(self, prompts):
        model = self.policy_model if self.policy_model else self.base_model
        responses = []
        
        for prompt in prompts:
            inputs = self.tokenizer(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=256,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            responses.append(response[len(prompt):].strip())
        
        return responses
    
    def train_initial_reward_model(self, preference_data):
        # Train reward model from scratch
        reward_model = RewardModel(self.base_model.config.name_or_path)
        
        # Prepare training data
        train_dataloader = self.prepare_reward_training_data(preference_data)
        
        optimizer = torch.optim.AdamW(reward_model.parameters(), lr=1e-5)
        
        reward_model.train()
        for epoch in range(3):  # Few epochs for reward model
            for batch in train_dataloader:
                optimizer.zero_grad()
                
                preferred_rewards = reward_model(
                    batch['preferred_input_ids'], 
                    batch['preferred_attention_mask']
                )
                dispreferred_rewards = reward_model(
                    batch['dispreferred_input_ids'],
                    batch['dispreferred_attention_mask']
                )
                
                loss = ranking_loss(preferred_rewards, dispreferred_rewards)
                loss.backward()
                optimizer.step()
        
        return reward_model
    
    def run_ppo_optimization(self):
        # Initialize PPO trainer
        ppo_trainer = PPOTrainer(
            model=self.policy_model if self.policy_model else self.base_model,
            reward_model=self.reward_model,
            tokenizer=self.tokenizer
        )
        
        # Run PPO training
        optimized_model = ppo_trainer.train(num_epochs=1)
        
        return optimized_model

# Usage example
rlhf = IterativeRLHF(base_model, tokenizer)

# Run multiple iterations
for iteration in range(5):
    metrics = rlhf.run_iteration(training_prompts, human_feedback_collector)
    print(f"Iteration {iteration + 1} metrics: {metrics}")
""", language='python')

with advanced_tabs[1]:
    st.subheader("üéØ Multi-Objective RLHF")
    
    st.markdown("""
    Multi-objective RLHF optimizes for multiple competing objectives simultaneously,
    such as helpfulness, harmlessness, and honesty.
    """)
    
    objectives = [
        {
            "objective": "Helpfulness",
            "description": "Providing useful and informative responses",
            "metrics": ["Task completion rate", "User satisfaction", "Information accuracy"]
        },
        {
            "objective": "Harmlessness", 
            "description": "Avoiding harmful or dangerous outputs",
            "metrics": ["Safety violations", "Bias detection", "Toxic content rate"]
        },
        {
            "objective": "Honesty",
            "description": "Being truthful and acknowledging uncertainty",
            "metrics": ["Factual accuracy", "Uncertainty expression", "Source reliability"]
        }
    ]
    
    for obj in objectives:
        with st.expander(f"üéØ {obj['objective']}"):
            st.markdown(obj['description'])
            
            st.markdown("**Key Metrics:**")
            for metric in obj['metrics']:
                st.markdown(f"‚Ä¢ {metric}")

with advanced_tabs[2]:
    st.subheader("üìä Online Learning in RLHF")
    
    st.markdown("""
    Online learning allows continuous model improvement during deployment
    by incorporating real-time user feedback.
    """)
    
    online_challenges = [
        "Distribution shift between training and deployment",
        "Balancing exploration vs exploitation",
        "Handling contradictory feedback", 
        "Maintaining model stability",
        "Computational efficiency constraints"
    ]
    
    st.markdown("**Key Challenges:**")
    for challenge in online_challenges:
        st.markdown(f"‚Ä¢ {challenge}")

with advanced_tabs[3]:
    st.subheader("üîß Debugging RLHF")
    
    st.markdown("""
    RLHF training can be unstable and difficult to debug. Here are common issues and solutions.
    """)
    
    debugging_issues = [
        {
            "issue": "Reward Hacking",
            "description": "Model exploits reward function flaws",
            "solutions": [
                "Diversify training data",
                "Use ensemble reward models", 
                "Add regularization terms",
                "Monitor for unexpected behaviors"
            ]
        },
        {
            "issue": "Training Instability",
            "description": "PPO training becomes unstable or diverges",
            "solutions": [
                "Lower learning rates",
                "Increase batch sizes",
                "Use gradient clipping",
                "Add KL divergence penalties"
            ]
        },
        {
            "issue": "Poor Generalization",
            "description": "Model performs well on training tasks but poorly on new ones",
            "solutions": [
                "Increase data diversity",
                "Use cross-validation",
                "Implement regularization",
                "Test on held-out datasets"
            ]
        }
    ]
    
    for issue in debugging_issues:
        with st.expander(f"üêõ {issue['issue']}"):
            st.markdown(issue['description'])
            
            st.markdown("**Solutions:**")
            for solution in issue['solutions']:
                st.markdown(f"‚Ä¢ {solution}")

# Best practices
st.header("üí° RLHF Best Practices")

best_practices = [
    "**Start Simple**: Begin with basic SFT before adding RLHF complexity",
    "**Quality Over Quantity**: High-quality human feedback is more valuable than large volumes",
    "**Diverse Evaluators**: Use multiple human annotators to capture different perspectives", 
    "**Regular Evaluation**: Continuously monitor model performance on held-out test sets",
    "**Iterative Approach**: Run multiple RLHF cycles rather than trying to perfect in one round",
    "**Safety First**: Prioritize safety and alignment over raw performance metrics",
    "**Documentation**: Maintain detailed records of training procedures and decisions",
    "**Stakeholder Involvement**: Include diverse stakeholders in the feedback process",
    "**Computational Planning**: RLHF is expensive - plan resources accordingly",
    "**Failure Analysis**: Systematically analyze and learn from failure cases"
]

for practice in best_practices:
    st.markdown(f"‚Ä¢ {practice}")

# Resources
st.header("üìö Learning Resources")

resources = [
    {
        "title": "Training Language Models to Follow Instructions with Human Feedback",
        "type": "Research Paper",
        "description": "Original InstructGPT paper introducing RLHF for LLMs",
        "difficulty": "Advanced"
    },
    {
        "title": "Constitutional AI: Harmlessness from AI Feedback", 
        "type": "Research Paper",
        "description": "Anthropic's approach to AI-assisted constitutional training",
        "difficulty": "Advanced"
    },
    {
        "title": "TRL (Transformer Reinforcement Learning)",
        "type": "Library",
        "description": "Hugging Face library for RLHF and PPO training",
        "difficulty": "Intermediate"
    },
    {
        "title": "Deep Reinforcement Learning from Human Preferences",
        "type": "Research Paper", 
        "description": "Foundational work on learning from human preferences",
        "difficulty": "Advanced"
    }
]

for resource in resources:
    with st.expander(f"üìñ {resource['title']}"):
        st.markdown(f"**Type:** {resource['type']}")
        st.markdown(f"**Description:** {resource['description']}")
        st.markdown(f"**Difficulty:** {resource['difficulty']}")